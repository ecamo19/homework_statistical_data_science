---
title: 'STAT 534 Statistical Data Science I: Exam I (Take-Home Portion)'
author: "Erick Calderon-Morales"
date: "Fall 2021"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE ,comment = "", fig.align = 'center',
					  fig.width = 12, fig.height = 6, warning = FALSE)
```

__Due Date:__ October 17 (Sunday)

__Instructions:__ There are 50 points plus 10 extra credits on this portion of the exam. You may use any reference material for your exam. However, you are not to discuss any questions about the exam with anyone other than yourself. Please contact me with any questions and I will respond as soon as possible within the same day. Be sure to write/type your answers in complete sentences and show your work. Any items including R output not commented on will receive no credit. Attach your R code as an appendix or as a separate file. (If you use R markdown, you may keep the code
inserted inline.)


```{r message=FALSE, warning=FALSE}
# Packages
library(tidyverse)
# For cleaning names
library(janitor)
# Best subset selection 
library(leaps)
# For tidy function
library(broom)
# For mse plot
library(ggvis)
# For UScrimes dataset
library(MASS)
# For correlations 
library(GGally)
# For joining plots
library(cowplot)
# For lasso and ridge regressions
library(glmnet)
# For pls and pcr regressions
library(pls)
# For tables
library(gt)
```

# Exercise 1
__1. (20+10 pts) In this problem, we will generate simulated data, and will then apply best subset selection to this data using the validation method. The goal is to explore that as model size increases, the training error will necessarily decrease, but the testing error may not.__

_(a) (5 pts) Generate a data set with p = 20 predictors and n = 1000 observations according to the model:_

$$Y = \beta_0\ +\ \beta_1X_1\ ...\ +\ \beta_pX_p\ +\ \epsilon $$

where $X_j \sim N(0,1)$  and $\epsilon\ \sim N (0, 1)$ independently. Randomly select your $\beta$ values but let some elements to be exactly zero. Then split your data set into a training set containing 100 observations and a testing set containing 900 observations.

```{r}
set.seed(123)

# Generate e values with mean 0 and sd 1
epsilon <- rnorm(1000, mean = 0, sd = 1)

# Generate x values with mean 0 and sd 1
n = 1000
variables = 20

# Create empty data frame
empty_data_set <- matrix(numeric(variables * n), 
                                    ncol = variables,
                                    nrow = n)

for (each_variable in seq(along = 1:variables)){
    
     # Get random data and append to data frame   
     empty_data_set[,each_variable] <- rnorm(1000, mean = 0, sd = 1)
}

# Clean data set
x_variables <- empty_data_set
   
```


```{r}
# Select randomly the betas and set some to zero
random_betas <- rnorm(variables, mean = 5, sd = 10)

# Set some to Zero
random_betas[5]  <- 0
random_betas[15] <- 0
random_betas[17] <- 0


# Generate Y using my betas and simulated data
y <-  x_variables %*% random_betas + epsilon
colnames(y) <- "y"
```

```{r}
# Join data'
data_set_full <- 
  cbind(y,as.data.frame(x_variables)) %>% 
  clean_names()

```


```{r}
# slip data into train and test

# Get index
train_index <- sample(1:nrow(data_set_full),900)

# Test set
data_set_train <- data_set_full[train_index,]
nrow(data_set_train)

# Train set
data_set_test <-  data_set_full[-train_index,]
nrow(data_set_test)
```
```{r}
model_matrix_train <- model.matrix(y ~ ., data = data_set_train)
model_matrix_test  <- model.matrix(y ~ ., data = data_set_test)
```

_(b) (3 pts) Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size. (Hint: regsubsets() returns error (or residual) sum of squares (rss) for each model and MSE = RSS/n.)_

```{r}
# Model on train data
best_subset_sel <- regsubsets(y ~.,
                              nvmax = 20,
                              method="exhaustive", 
                              nbest=1,
                              data = data_set_train)

best_subset_sel_summary <- summary(best_subset_sel)
```


```{r}
# Get rss and change the colname
models_rss <- as.data.frame(best_subset_sel_summary$rss)
colnames(models_rss) <- "rss"

# Calculate MSE
mse_train_set <- models_rss$rss/nrow(data_set_train)

# Join data
models_errors <- cbind(models_rss,mse_train_set)
```

```{r}
ggplot(data = models_errors, aes(x = c(1:20), y = mse_train_set, 
                                 color = mse_train_set))+
    geom_point() +
    ylab("Mean squared error train set") + xlab("Number of variables") +
    theme_bw()
```

_(c) (6pts) Plot the testing set MSE associated with the best model of each size. For which model size does the testing set MSE take on its minimum value? (Hint: For each model, obtain the predicted values for the testing set and then compute associated MSE.)_

```{r}
# Create empty vector
mse_test_set <- rep (NA , 20)

for (each_model in 1:20) {
  
    # Get the coefficients of each model build with the train set
    coefs <- coef(best_subset_sel, id = each_model)
    
    # get model variables and multiply then for their coef for getting the pred
    pred <- model_matrix_test[,names(coefs)] %*% coefs
    
    # get mse
    mse_test_set[each_model]<- mean((data_set_test$y - pred)^2)
}

# Add vector to a mse errors data frame 
mse_test_set <- as.data.frame(mse_test_set)

models_errors <- cbind(models_errors,mse_test_set)

```

```{r}
ggplot(data = models_errors, aes(x = c(1:20), y = mse_test_set, 
                                color = mse_test_set)) +
    geom_point() +
    ylab("Mean squared error test set") + xlab("Number of variables") +
    theme_bw()
```
_(d) (3 pts) What do you observe about the changes in training MSE and testing MSE as model size increases?_
```{r}
ggplot(data = models_errors) +
    geom_point(aes(x = c(1:20), y = mse_train_set, color = "MSE Train")) +
    geom_point(aes(x = c(1:20), y = mse_test_set, color =  "MSE Test")) +
    ylab("Mean squared error test set") + xlab("Number of variables") +
    theme_bw()
```
```{r}

plot(best_subset_sel_summary$cp ,xlab =" Number of Variables ", ylab =" Cp",type="p")
plot(best_subset_sel_summary$bic,xlab =" Number of Variables ", ylab =" BIC ",type="p")
```



_(e) (3 pts) How does the model at which the testing MSE is minimized compare to the true model used to generate the data? (Hint: You want to refit the regression model to the entire data set using the selected predictors.)_
```{r}
coef(best_subset_sel, id = 17)
```
```{r}
# Refit the regression model to the entire data using the selected predictors

# Get MSE from original model
model_true <- lm(y ~ ., data = data_set_full)
model_true_pred <- predict(model_true)
(mse_model_true <- mean((y - model_true_pred)^2))

# Get MSE from model chosen

model_selected <- lm(y ~ v1  + v2  + v3  + v4  + v6  + v7  + 
                         v8  + v9  + v10 + v11 + v12 + v13 + 
                         v14 + v16 + v18 + v19 + v20,
                     data = data_set_full)

model_selected_pred <- predict(model_selected)
(mse_model_selected <- mean((y - model_selected_pred)^2))
```

_(f) (+10 pts) Create a plot displaying $\sqrt{\sum_{j=0}^p(\beta_j - \hat \beta_j^s}^2$ where $\hat \beta_j^s$ is the jth coefficient estimate for the best model of size s using the entire data set. Comment on what you observe.How does this compare to the testing MSEb plot from part (c)?_

https://www.kaggle.com/mahmoud86/tutorial-subset-selection-methods
```{r}
coefficients <-
  # Get the original betas
  data.frame(parameter = colnames(best_subset_sel_summary$which), 
                           actual = c(0, random_betas)) %>%
  
  # Get estimated betas
  left_join(data.frame(parameter = names(coef(best_subset_sel, id = 17)), 
                       estimated = coef(best_subset_sel, id = 17)), 
            by = "parameter") %>%
  # Set to 0 the estimated betas that were not estimated
  mutate(estimated = case_when(is.na(estimated) ~ 0, TRUE ~ estimated))

```

# Exercise 2

__2. (30 pts) Use the UScrime data set in the MASS library to study the effect of punishment regimes on crime rates.__

```{r}
set.seed(123)

# Load data
data("UScrime")

uscrime <- 
  UScrime %>% 
    clean_names()

summary(uscrime) 
```

_(a) (5 pts) Explore the variables using appropriate graphics and summary statistics. Comment on your observations._

```{r}
# Nice visualization of correlations
correlations <- 
  uscrime %>%
    ggcorr(geom = "blank", label = TRUE, hjust = 0.75) +
    geom_point(size = 10, aes(color = coefficient > 0, 
                              alpha = abs(coefficient) > 0.5)) +
    scale_alpha_manual(values = c("TRUE" = 0.25, "FALSE" = 0)) +
    guides(color = FALSE, alpha = FALSE)
```

```{r}
y_density <- ggplot(uscrime, aes(x = y)) + geom_density()
```

```{r}
cowplot::plot_grid(y_density,correlations)
```
From the plot above I observed high correlation between variables. Especially the percentage of males aged 14–24 (m), the indicator variable for a Southern state (so), gross domestic product per capita (gdp) and income inequality (ineq) are highly correlated with other variables.

From the variables above its worth to mention that the percentage of males aged 14–24 (m) is positively correlated with the variables Southern States (so),number of non-whites per 1000 people (nw) and income inequality. While is negatively correlated with the variables mean years of schooling (Ed), police expenditure in 1960 (Po1) police expenditure in 1959 (Po2) and gdp. While the variable income inequality is negatively correlated with gdp, Po1, Po2 and ed and positively correlated with nw, so and m.


```{r}
uscrime <- 
  uscrime %>% 
    mutate(so = factor(so),
           # log transform y variable
           log_y = log(y)) %>% 
    dplyr::select(-y)
```
 

_(b) (12 pts) Split the data into 75% training and 25% testing and build the following models using the training set:_


```{r}
# slip data into train and test

# Get index
train_index <- sample(1:nrow(uscrime),(nrow(uscrime)*75)/100)

# Train set
uscrime_train <- uscrime[train_index,]
(nrow(uscrime_train)*100)/nrow(uscrime)

# This is done for fittin ridge and lasso
x_vars_train <- model.matrix(log_y ~., data = uscrime_train)[,-1]
y_var_train <- uscrime_train$log_y 

# Test set
uscrime_test <-  uscrime[-train_index,]
(nrow(uscrime_test)*100)/nrow(uscrime)
y_var_test <- uscrime_test$log_y 


# This is done for fitting ridge and lasso
x_vars_test <- model.matrix(log_y ~., data = uscrime_test)[,-1]

```


+ Multiple linear regression
```{r}
mult_reg_train <- regsubsets(log_y ~., nvmax = 16, 
                             nbest = 1,
                             method="exhaustive", 
                             data = uscrime_train)

mult_reg_train_summary <- summary(mult_reg_train)
```

```{r}
# Select best multiple regression model
plot(mult_reg_train_summary$cp , xlab =" Number of Variables ", ylab =" Cp",type="p")
plot(mult_reg_train_summary$bic, xlab =" Number of Variables ", ylab =" BIC ",type="p")
```
```{r}
coef(mult_reg_train, id = 6)
```

```{r}
# Refit the model
best_mult_reg_train <- lm(log_y ~ m + so + ed + po1 + ineq + time,  
                          data = uscrime_train)
```



+ Ridge regression
```{r}
ridge_uscrimes_train <- cv.glmnet(x_vars_train, y_var_train, alpha = 0)
```

```{r}
plot(ridge_uscrimes_train)
```


+ Lasso regression
```{r}
lasso_uscrimes_train <- cv.glmnet(x_vars_train, y_var_train, alpha = 1)
```

```{r}
plot(lasso_uscrimes_train)
```

+ Principal components regression (justify how many principal components should be used)
```{r}
pcr_uscrimes_train <- pcr(log_y ~ ., data = uscrime_train, 
                          scale = TRUE , 
                          validation = "CV")
```


```{r}
validationplot(pcr_uscrimes_train, val.type = "MSEP", type = "p")
```
I choose to use 5 components for the principal component regression since the value of MSEP start to stabilize after 5 components 


+ Partial least squares (justify how many directions should be used)
```{r}
pls_uscrimes_train <- plsr(log_y ~ ., data = uscrime_train, 
                          scale = TRUE , 
                          validation = "CV")
```

```{r}
validationplot(pls_uscrimes_train, val.type = "MSEP", type = "p")
```
I choose to use 3 components for the partial least squared regression since it has the lowest MSEP value.


_(c) (5 pts) Compare the effectiveness of each model on training vs. testing data._

 
+ Multiple linear regression MSE
```{r}
# Training error
mult_mse_train <- summary(best_mult_reg_train)$sigma^2

# Testing error
mult_pred <- predict(best_mult_reg_train, newx = uscrime_test)

mult_mse_test <- mean((mult_pred - y_var_test)^2)
```

+ Ridge regression MSE
```{r}
# MSE train
ridge_pred_train <- predict(ridge_uscrimes_train, 
                            s = ridge_uscrimes_train$lambda.1se,
                            newx = x_vars_train)

# Training error
ridge_mse_train <- mean((ridge_pred_train - y_var_train)^2) 

# MSE test
ridge_pred_test <- predict(ridge_uscrimes_train, 
                            s = ridge_uscrimes_train$lambda.1se,
                            newx = x_vars_test)
# Test error
ridge_mse_test <- mean((ridge_pred_test - y_var_test)^2) 
```

+ Lasso regression MSE
```{r}

# MSE train
lasso_pred_train <- predict(lasso_uscrimes_train, 
                            s = lasso_uscrimes_train$lambda.1se,
                            newx = x_vars_train)

# Training error
lasso_mse_train <- mean((lasso_pred_train - y_var_train)^2) 

# MSE test
lasso_pred_test <- predict(lasso_uscrimes_train, 
                            s = lasso_uscrimes_train$lambda.1se,
                            newx = x_vars_test)
# Test error
lasso_mse_test <- mean((lasso_pred_test - y_var_test)^2) 
```

+ PCR MSE
```{r}
# Train error
pcr_pred_train <- predict(pcr_uscrimes_train, data = uscrime_train, ncomp = 5)
pcr_mse_train <- mean((pcr_pred_train - y_var_train)^2)

# Test error
pcr_pred_test <- predict(pcr_uscrimes_train, uscrime_test, ncomp = 5)
pcr_mse_test <- mean((pcr_pred_test - y_var_test)^2)
```

+ PLSR MSE
```{r}
# Train error
pls_pred_train <- predict(pls_uscrimes_train, data = uscrime_train, ncomp = 3)
pls_mse_train <- mean((pls_pred_train - y_var_train)^2)

# Test error
pls_pred_test <- predict(pls_uscrimes_train, uscrime_test, ncomp = 3)
pls_mse_test <- mean((pls_pred_test - y_var_test)^2)
```



_Table 1: Models MSE_
```{r}
tribble(
~model, ~mse_train, ~mse_test,~difference,
"MLR",   mult_mse_train,  mult_mse_test,  abs(mult_mse_test  - mult_mse_train),
"RIDGE", ridge_mse_train, ridge_mse_test, abs(ridge_mse_test - ridge_mse_train),
"LASSO", lasso_mse_train, lasso_mse_test, abs(lasso_mse_test - lasso_mse_train),
"PCR",   pcr_mse_train,   pcr_mse_test,   abs(pcr_mse_test   - pcr_mse_train),
"PLS",   pls_mse_train,   pls_mse_test,   abs(pls_mse_test   - pls_mse_train)
) %>% gt()
```

Based on which model has the lowest mse test value and the lowest difference between mse test and mse train I decided to choose the Ridge model and the Principal component regression model with 5 components.  

_(d) (8 pts) Select the best two models from above. Interpret and compare their respective final fitted models._

+ Refit Ridge model to the entire data set
```{r}
x_variables_uscrime <- model.matrix(log_y ~., data = uscrime)[,-1]
y_variable_uscrime <- uscrime$log_y

ridge_uscrimes <- cv.glmnet(x_variables_uscrime, y_variable_uscrime, 
                                  alpha = 0)

coef(ridge_uscrimes)
```

+ Refit PCR model to the entire data set
```{r}
pcr_uscrimes <- pcr(log_y ~ ., ncomp = 5,
                          scale = TRUE ,
                          data = uscrime, 
                          validation = "CV")
summary(pcr_uscrimes)
pcr_uscrimes[["coefficients"]][,,1]
```

