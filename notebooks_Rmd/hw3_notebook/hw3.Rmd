---
title: 'STAT 534: Homework 3'
author: "Erick Calderon-Morales"
date: ' Fall 2021'
due_date: ""
output: pdf_document
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,comment = "", fig.align = 'center',
					  fig.width =  15, fig.height = 11)
```



```{r knitr, include = FALSE}

# Save figures in specific place

knitr::opts_chunk$set(autodep        = TRUE,
                      cache          = FALSE,
                      cache.comments = TRUE,
                      
                      # Include code?
                      echo           = TRUE,
                      
                      error          = FALSE,
                      fig.align      = "center",
                      
                      # Path to where to store pdf single figures 
                      fig.path       = paste0("hw3_figures", "/"),
                      fig.width      = 11,
                      fig.height     = 7,
                      message        = FALSE,
                      warning        = FALSE)
```


```{r cleanup-docs, cache = FALSE,echo = FALSE}

# save a html copy file in a specific place
doc.files <- c(list.files(pattern = "pdf"),
               list.files(pattern = "html"),
               list.files(pattern = "docx"))

for (file in doc.files) {
    file.rename(file, file.path("../../hw3/", file))
}
```


```{r libaries, message=FALSE, warning=FALSE, cache=FALSE}
library(leaps)
library(gt)
library(tidyverse)
library(glmnet)
library(janitor)
library(MASS)
library(ISLR)
library(NHANES)
library(broom)
library(rsample)
library(caret)
library(GGally)
```

# Exercise 1 

__In this problem, we will generate simulated data, and will then use this data to perform variable selection.__

__(a) Use the rnorm() function to generate a predictor X of length n = 100, as well as a noise vector $\epsilon$ of the same length. Then generate a response vector Y according to the model:__

$$Y =  \beta_0 + \beta_1 X + \beta_2X^2 + \beta_3X^3 + \epsilon $$
__where $\beta_0$,..., $\beta_3$ are constants of your choice.__

```{r}
x <- rnorm(100)
e <- rnorm(100)
```

```{r}
y <- 9 + 1 * x + 2 * x^2 - 3 * x^3 + e
```


__(b) Given the predictors $X$, $X^2$ , ..., $X^{10}$ , perform best subset selection in order to choose the best model. What is the best model obtained according to Cp, AIC, BIC and adjusted $R^2$ ? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained.__


```{r}

# Get AIC values
poly_degree <- seq(1, 10)

# Empty vector
aic <- double(length(poly_degree))

for (each_value in seq(along = poly_degree)) {
 k <- poly_degree[each_value]
 
 # Polynomial Model
 aic_out <- lm(y ~ poly(x, k))
 
 # Assign it to vector
 aic[each_value] <- AIC(aic_out)
}

aic_values <- cbind(poly_degree,aic)
```

```{r}
data_from_linear_model <- data.frame(y,x)

# Generate polynomial regression up to 10
fit <- regsubsets(y ~ poly(x, 10), data = data_from_linear_model, nvmax = 10)
fit_summary <- summary(fit)
```


```{r}
# Choose best model
metrics <- data.frame(
  r2 = which.max(fit_summary$adjr2),
  cp = which.min(fit_summary$cp),
  BIC = which.min(fit_summary$bic),
  aic = which.min(aic_values[,2])
)
```


```{r}
# Generate data frame with metrics

data_model_selection <- 
  data_frame(cp = fit_summary$cp,
           BIC = fit_summary$bic,
           r2 = fit_summary$adjr2) %>%
    
    # add aic values
    cbind(., aic_values[,2]) %>% 
    rename(AIC = "aic_values[, 2]") %>% 
  
    mutate(id = row_number())
```


```{r}
data_model_selection %>% 
    
    #Transform to long format
    gather(value_type, value, -id) %>% 
    ggplot(aes(id, value, col = value_type)) +
    geom_line() + 
    geom_point() + 
    ylab('') + 
    xlab('Number of Variables Used') +
    facet_wrap(~ value_type, scales = 'free') +
    theme_bw() + 
    scale_x_continuous(breaks = 1:10) +
    # Change color
    scale_colour_manual(values = c("#d8b365", "#0072B2", "#5ab4ac",
                                   "#56B4E9")) +
      
    # Edit the legend
    theme(axis.text.y   = element_text(size = 14),
                # Legend position and Axis size 
                legend.position = "bottom",
                axis.text.x   = element_text(size = 14),
    			  	  axis.title.y  = element_text(size = 14),
    			  	  axis.title.x  = element_text(size = 14),
                # Add borders to the plot
    			  	  panel.border = element_rect(colour = "black", fill= NA,size = 1.3)) +
    
      # Edit legend name
      labs(colour = "Criteria") +  
      
      #Edit legend 
      guides(col = guide_legend(override.aes = list(fill=NA),nrow = 1,title.position = "top",))

```

According to the plots, the model with 3 variables is the best



```{r}
best_model <- lm(y ~ poly(x, 3), data = data_from_linear_model)
coef(best_model)
```


__(c) Repeat (b), using forward selection. How does your answer compare to the results in (b)?__


```{r}

fit_forward <- regsubsets(y ~ poly(x, 10), data = data_from_linear_model, 
                          nvmax = 10, method = "forward")
fit_summary_forward <- summary(fit_forward)

```


```{r}

# Choose best model
metrics_forward <- data.frame(
  r2 = which.max(fit_summary_forward$adjr2),
  cp = which.min(fit_summary_forward$cp),
  BIC = which.min(fit_summary_forward$bic)
)
```

```{r}
metrics %>% gt()
metrics_forward %>% gt()
```


In this case, the forward procedure did not produced any different results 

__(d) Now fit a lasso model and use cross-validation to select the optimal values of $\lambda$. Create plots of the cross-validation error as a function f of $\lambda$. Report the resulting coefficient estimates, and discuss the results obtained__


```{r}
# Generate data

data_lasso <- data.frame(cbind(y,x,x^2,x^3,x^4,x^5,x^6,x^7,x^8,
                     x^9,x^10)) %>% clean_names()
  
x <- model.matrix(y ~. ,data_lasso)[, -1]
y <- data_lasso[,1]

```

```{r}
# split the samples into a training set and a test set

train <- sample(1:nrow(x),nrow(x) / 2)
test <- (-train)
y_test <- y[test]
```

```{r}
#cross-validation to select the optimal values of 
cv_out <- cv.glmnet(x[train,] , y[train], alpha = 1)
```

```{r}
best_lamb <- cv_out$lambda.min
```

```{r}
# Perform lasso regression
lasso_mod <- glmnet(x[train,] , y[train] , alpha = 1)
lasso_pred <- predict(lasso_mod , s = best_lamb ,newx = x[test,])

```


```{r}
# Create plots of the cross-validation error as a function f of 
plot(cv_out)
```

```{r}
lasso_coef <- predict(lasso_mod, type = "coefficients",s = best_lamb)[1:11,]
lasso_coef
```

From the lasso model we can see that the variables greater than 0 have a significant effect over the _y_ variable 


# Exercise 2 

```{r}
rm(x,y)
```

__The ability to get a good night’s sleep is correlated with many positive health outcomes. Use the NHANES data set from the NHANES package to predict _SleepHrsNight_. Check the R help document for detailed information about the data set.__

```{r}
# Load Data
data("NHANES")
data_p2 <- 
    NHANES %>% 
    clean_names()
```

__(b) Select your own predictors, and create plots or summary tables to explore the variables.__

```{r}
data_selected_var <- data_p2 %>%
  dplyr::select(sleep_hrs_night,gender,age,race1,education,poverty) %>% 
  na.omit()
```


```{r}
ggpairs(data_selected_var,upper = "blank")
```


__(a) First separate the data set at random into 75% training and 25% testing sets.__

```{r}
# Select X-Y for models
x_sleep <- model.matrix(sleep_hrs_night ~ . ,data_selected_var)[, -1]

y_sleep <- data_selected_var$sleep_hrs_night

# Sample data
index <-  sample(1:nrow(x_sleep), 0.75 *nrow(x_sleep)) 

# Create the training data 75% 
train_data = x_sleep[index,]  
(nrow(train_data)/6671)*100

# Create the test data 25% 
test_data = x_sleep[-index,] 
(nrow(test_data)/6671)*100


```

__(c) Build the following models using the training set with your predictors of choice:__

+ Multiple linear regression

```{r}
m1 <- lm(y_sleep[train] ~ x_sleep[train,])
```


+ Ridge regression
```{r}
# Model
m2_ridge <- glmnet(x_sleep[train, ], y_sleep[train] ,alpha = 0)

# Get best lambda
lambda_sleep_ridge <- cv.glmnet(x_sleep[train,], y_sleep[train], alpha = 0)

best_lamb_ridge <- lambda_sleep_ridge$lambda.min
```

+ LASSO regression
```{r}
# Model
m3_lasso <- glmnet(x_sleep[train , ], y_sleep[train] ,alpha = 1)


# Get best lambda
lambda_sleep_lasso <- cv.glmnet(x_sleep[train,], y_sleep[train], alpha = 1)

best_lamb_lasso <- lambda_sleep_lasso$lambda.min
```


__(d) Compare the effectiveness of each model on training vs. testing data.__

I found that the MSE were generally low, indicating the effectiveness of each model on 
training vs testing data

+ Linear regression MSE

```{r}
linear_prediction <- predict(m1,newx = x_sleep[test_data, ])

y_sleep_test <- y_sleep[test_data]
mean((linear_prediction - y_sleep_test)^2)
```

+ Ridge MSE 


```{r}
ridge_pred <- predict(m2_ridge , s = best_lamb_ridge, newx = x_sleep[test_data,])
mean((ridge_pred - y_test)^2)
```

+ Lasso MSE

```{r}
lasso_pred <- predict(m3_lasso , s = best_lamb_lasso, newx = x_sleep[test_data,])
mean((lasso_pred - y_test)^2)
```


__(e) Choose one best model and interpret the results. What have you learned about people’s sleeping quality?__

Based on the MSE I chose the Multiple linear regression model because it has the lowest MSE(~5)

```{r}
model <- lm(sleep_hrs_night ~ .,  data = data_selected_var )
```

```{r}
summary(model)
anova(model)
```


From the variables selected, all had an effect over the amount of sleeping hours. For example it seems non-white people sleeps less when compared to white people. 

```{r}
ggplot(data = data_selected_var, aes(x = age, y = race1, fill = race1))+
  geom_boxplot() + theme_bw() 
```

