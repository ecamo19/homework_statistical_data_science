---
title: 'STAT 534: Homework 5 '
author: "Erick Calderon-Morales"
date: ' Fall 2021'
due_date: ""
output:
  prettydoc::html_pretty:
    highlight: pygments
    theme: cayman
    toc: yes
    number_sections: no
    toc_depth: 1
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,comment = "", fig.align = 'center',
					  fig.width = 11, fig.height = 7)
```

```{r knitr, include = FALSE}

# Save figures in specific place

knitr::opts_chunk$set(autodep        = TRUE,
                      cache          = FALSE,
                      cache.comments = TRUE,
                      
                      # Include code?
                      echo           = TRUE,
                      
                      error          = FALSE,
                      fig.align      = "center",
                      
                      # Path to where to store pdf single figures
                      fig.path       = paste0("../hw5_notebook/hw5_figures", "/"),
                      fig.width      = 11,
                      fig.height     = 7,
                      message        = FALSE,
                      warning        = FALSE)
```


```{r cleanup-docs, cache = FALSE,echo = FALSE}

# save a html copy file in a specific place
doc.files <- c(list.files(pattern = "pdf"),
               list.files(pattern = "html"),
               list.files(pattern = "docx"))

for (file in doc.files) {
    file.rename(file, file.path("../../hw5/", file))
}
```


```{r libaries, message=FALSE, warning=FALSE, cache=FALSE}
library(mdsr)
library(tidyverse)
library(tidymodels)
library(yardstick)
library(gt)
# for random forest
library(randomForest)
# for calculating sensitivity and specificity
library(caret)
# for ROC curve
library(pROC)
# for joining plots
library(cowplot)
```

_1. Load “titanic data.csv” data downloaded from the Canvas. Use the following codes to preprocess the data:_

```{r message=FALSE, warning=FALSE}

filename <- "../../hw5/data/titanic_data.csv"
 
titanic <- 
    read_csv(filename) %>%
        dplyr::select(-c(home.dest, cabin, name, x, ticket))%>%
        filter(embarked != "?")%>%
      
        mutate(pclass = factor(pclass, levels = c(1, 2, 3), 
                               labels = c("Upper", "Middle", "Lower")),
      
        survived = factor(survived, levels = c(0, 1), 
                   labels = c("No", "Yes")),
                   age = as.numeric(age),
                   sex = factor(sex),
                   embarked = factor(embarked)) %>%
        # No NA's in the data set
        na.omit() 
        
glimpse(titanic)
```

_Count total of No and Yes in the dataset_

How do I know if the dataset is imbalanced? 

```{r}
titanic %>% 
  group_by(survived) %>% 
  count() %>%   
  mutate(percent = (n/nrow(titanic)*100))
```


_(a) Separate the training and testing datasets using 80%/20% splitting. (5 points)_

```{r}
# Training testing/split
set.seed(666)

n <- nrow(titanic)

# Split data 80/20
index <- sample(1:nrow(titanic),(nrow(titanic)*.80)/1)

# Get training data
training_data <- titanic[index,]

# Get testing data
testing_data <- titanic[-index,]

```
```{r}
# Double check
(nrow(training_data)*100)/nrow(titanic)
(nrow(testing_data)*100)/nrow(titanic)
```


_(b) Fit a logistic regression model to predict survived using all other available variables. Here are the formula:_

$$survived\ \sim\ pclass + sex + age + sibsp + parch + fare + embarked$$
```{r}
# Model formula 
model <- as.formula(survived ~ pclass + sex  + age + sibsp + 
                               parch  + fare + embarked)
```


```{r}
# Multivariate logistic regression
model_logit <-
    # Class
    logistic_reg(mode = "classification") %>%
    
    # Engine
    set_engine("glm") %>%
    
    # Fit formula    
    fit(model,
        data = training_data)
```

_(b.1) Please show the confusion matrix for the training data and calculate the training accuracy rate. (10 points)_

__The confusion matrix shows that the logistic regression model in the train dataset classified correctly as No 427 out of 524 observations while it classified correctly as Yes 237 out of 310 observations having a accuracy of ~ 80%.__ 

```{r}
# Get training predictions
pred_train_logit <- 
  training_data %>% 
      bind_cols(predict(model_logit, new_data = training_data)) %>%
      rename(survived_pred = .pred_class)

# Confusion matrix
pred_train_logit %>%
  conf_mat(truth = survived, estimate = survived_pred)
```

```{r}
# Training accuracy rate
accuracy(pred_train_logit, survived, survived_pred) 
```



_(c) If we want to fit a random forests model using the same formula, how to set the parameter mtry (the number of candidates for splitting varibles)? (5 points)_

__For setting the parameter _mtry_ for the random forest it is recommend to use $\sqrt{predictors}$.__


_(d) Fit a random forests model and show the confusion matrix for the training data and calculate the training accuracy rate. You can set the number of trees as 201. (10 points)._


__The confusion matrix shows that the random forest model in the train dataset classified correctly as No 436 out of 539 observations while it classified correctly as Yes 231 out of 295 observations having a accuracy of ~ 80%.__ 


```{r}
random_forest_titanic <- randomForest (model, data = training_data,
         
                                       # mtry = squared of p
                                       #  -1 = remove response variable
                                       mtry = 
                                         round(sqrt(ncol(training_data) - 1),0), 
                                       
                                       # Number of trees
                                       ntree = 201,
                                       
                                       # Which variables have the greatest 
                                       # importance?
                                       importance = TRUE)

```

```{r}
# Get training predictions
pred_train_random_forest <- 
  training_data %>%
      bind_cols(predict(random_forest_titanic, new_data = titanic[index,])) %>% 
      rename(survived_pred_random_forest = ...9)

# Confusion matrix
pred_train_random_forest %>%
  conf_mat(truth = survived, estimate = survived_pred_random_forest)
```

```{r}
# Training accuracy rate
accuracy(pred_train_random_forest, survived, survived_pred_random_forest)
```

_(e) Find the most important explanatory variable of the random forests model. (5 points)_

__From the plots below, it seems that sex it's the most important variable for the classification of survived, while fare and age appear to be also important when taking into consideration only the Gini index.__

```{r}
varImpPlot(random_forest_titanic)
```
 
_(f) Find the testing accuracy rate of the two models. (5 points)_

__The testing accuracy for the logistic regression is 76% and the testing accuracy for the random forest is ~79%.__ 

```{r}
# Get test predictions for logistic regression model
pred_test_logit <- 
  testing_data %>% 
      bind_cols(predict(model_logit, new_data = testing_data)) %>%
      rename(survived_pred_test = .pred_class)
```

```{r}
# Test accuracy rate for logistic regression model
accuracy(pred_test_logit, survived, survived_pred_test)
```

```{r message=FALSE, warning=FALSE}
# Get test predictions for random forest model
pred_test_random_forest <- 
  testing_data  %>% 
      bind_cols(predict(random_forest_titanic, newdata = titanic[-index,], 
                        type = "class")) %>% 
      rename(survived_pred_random_forest_test = ...9)
      
```

```{r}
# Test accuracy rate for random forest model
accuracy(pred_test_random_forest, survived, survived_pred_random_forest_test)
```

_(g) Which accuracy should we use to compare the two models? What’s your conclusion? (5 points)_

__For comparing the two models the testing accuracy should be used. This because using the testing accuracy will indicate how well a model performs when new data is used. In this particular case, it seems that the random forest model is a little bit better than the logistic regression for classifing the survival.__

_(h) Calculate the sensitivity and specificity of the two models for the testing data. (5 points)_

Definition: "The sensitivity of the model is the rate that the event of interest is predicted correctly for all samples having the event. (True positive rate)
It measures the accuracy in the event population. Specificity, on the other hand, is to measure what portion of the actual false records you predicted correctly"

```{r}

pred_logit <- predict(model_logit, new_data = titanic[-index,])
colnames(pred_logit) <- "pred_logit"

testing_data <- cbind(testing_data,pred_logit)

```

```{r}
testing_data$random_forest_pred <- predict(random_forest_titanic, 
                                           newdata = titanic[-index,])
```

__When compared the two models, the sensitivity of the random forest is higher to the sensitivity of the logistic regression. This indicate that the random forest model has a higher capacity of detecting correctly the Yes in the variable survived than the logistic regression. In the case of specificity, this index is also higher in the random forest model. This indicate that the random forest model has also a higher capacity of detecting correctly the No in the variable survived.__ 

__Also, is important to note that in both models the Sensitivity is low (~60%). I think this happens because the data set is imbalance. There are more No (60%) cases that Yes (40%) which could led to low Sensitivity values.__

+ __Sensitivity for random forest model__

```{r}
# Class Yes will be used as the event of interest
sensitivity(data = testing_data$random_forest_pred, 
            reference = testing_data$survived,
            positive = "Yes")
```

+ __Sensitivity for logistic regression model__

```{r}
# Class Yes will be used as the event of interest
sensitivity(data = testing_data$pred_logit, 
            reference = testing_data$survived,
            positive = "Yes")
```

+ __Specificity for random forest model__

```{r}
specificity(data = testing_data$random_forest_pred,
            reference = testing_data$survived,
            negative = "No")
```

+ __Specificity for logistic regression model__

```{r}
specificity(data = testing_data$pred_logit,
            reference = testing_data$survived,
            negative = "No")
```

_(i) (Bonus) Draw the ROC curves of the two models using the testing data. (Hint: when you modify the demo codes, remember to replace ‘.pred >50K‘ with ‘.pred Yes‘.) (5 points)_


+ __ROC for random forest model__
```{r}
# Get probs for yes and no
probs_rf <- predict(random_forest_titanic, newdata = titanic[-index,], 
                    type = "prob")

testing_data$random_forest_prob_yes <- probs_rf[,2]

```

```{r message=FALSE, warning=FALSE}
roc_curve_rf <- roc(response  = testing_data$survived,
                    predictor = testing_data$random_forest_prob_yes,
                    levels    = rev(levels(testing_data$survived)))
roc_curve_rf$auc
```

+ __ROC curve for logistic regression model__
```{r}
logit_prob_yes <- predict(model_logit, new_data = titanic[-index,], 
                    type = "prob")[,2]

colnames(logit_prob_yes) <- "probs_logit_yes"

testing_data <- cbind(testing_data,logit_prob_yes)

```

```{r message=FALSE, warning=FALSE}
roc_curve_logit <- roc(response  = testing_data$survived,
                       predictor = testing_data$probs_logit_yes,
                       levels    = rev(levels(testing_data$survived)))
roc_curve_logit$auc
```

__Both ROC curves are very similar, the only difference is the area under the curve for the Random forest model (0.844) is a little bit greater that the area under the curve for the logistic model (0.8302)__
```{r}
par(mfrow = c(1,2))
plot(roc_curve_rf, legacy.axes = TRUE, main = "ROC for Random forest model")
plot(roc_curve_logit, legacy.axes = TRUE, main = "ROC for Logistic regression model")
```


