---
title: 'STAT 534: Homework 5 '
author: "Erick Calderon-Morales"
date: ' Fall 2021'
due_date: ""
output:
  prettydoc::html_pretty:
    highlight: pygments
    theme: cayman
    toc: yes
    number_sections: no
    toc_depth: 1

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,comment = "", fig.align = 'center',
					  fig.width = 11, fig.height = 7)
```

```{r knitr, include = FALSE}

# Save figures in specific place

knitr::opts_chunk$set(autodep        = TRUE,
                      cache          = FALSE,
                      cache.comments = TRUE,
                      
                      # Include code?
                      echo           = TRUE,
                      
                      error          = FALSE,
                      fig.align      = "center",
                      
                      # Path to where to store pdf single figures
                      fig.path       = paste0("../hw5_notebook/hw5_figures", "/"),
                      fig.width      = 11,
                      fig.height     = 7,
                      message        = FALSE,
                      warning        = FALSE)
```


```{r cleanup-docs, cache = FALSE,echo = FALSE}

# save a html copy file in a specific place
doc.files <- c(list.files(pattern = "pdf"),
               list.files(pattern = "html"),
               list.files(pattern = "docx"))

for (file in doc.files) {
    file.rename(file, file.path("../../hw5/", file))
}
```


```{r libaries, message=FALSE, warning=FALSE, cache=FALSE}
library(mdsr)
library(tidyverse)
library(tidymodels)
library(yardstick)
library(gt)
# for random forest
library(randomForest)
```

__1. Load “titanic data.csv” data downloaded from the Canvas. Use the following codes to preprocess the data:__

```{r message=FALSE, warning=FALSE}

filename <- "../../hw5/data/titanic_data.csv"
 
titanic <- 
    read_csv(filename) %>%
        dplyr::select(-c(home.dest, cabin, name, x, ticket))%>%
        filter(embarked != "?")%>%
      
        mutate(pclass = factor(pclass, levels = c(1, 2, 3), 
                               labels = c("Upper", "Middle", "Lower")),
      
        survived = factor(survived, levels = c(0, 1), 
                   labels = c("No", "Yes")),
                   age = as.numeric(age),
                   sex = factor(sex),
                   embarked = factor(embarked)) %>%
        na.omit() 
        
glimpse(titanic)
```

__(a) Separate the training and testing datasets using 80%/20% splitting. (5 points)__

```{r}
# Training testing/split
set.seed(666)

n <- nrow(titanic)

# Split data 80/20
index <- sample(1:nrow(titanic),(nrow(titanic)*.80)/1)

# Get training data
training_data <- titanic[index,]

# Get testing data
testing_data <- titanic[-index,]

# Double check
(nrow(training_data)*100)/nrow(titanic)
(nrow(testing_data)*100)/nrow(titanic)
```


__(b) Fit a logistic regression model to predict survived using all other available variables. Here are the formula:__

$$survived\ \sim\ pclass + sex + age + sibsp + parch + fare + embarked$$
```{r}
# Model formula 
model <- as.formula(survived ~ pclass + sex + age + sibsp + parch + fare + 
                        embarked)
```


```{r}

# Multivariate logistic regression
model_logit <-
    # Class
    logistic_reg(mode = "classification") %>%
    
    # Engine
    set_engine("glm") %>%
    
    # Fit formula    
    fit(model,
        data = training_data)
```

__(b.1) Please show the confusion matrix for the training data and calculate the training accuracy rate. (10 points)__
```{r}
# Get training predictions
pred_train_logit <- 
  training_data %>% 
      bind_cols(predict(model_logit, new_data = training_data)) %>%
      rename(survived_pred = .pred_class)

# Confusion matrix
pred_train_logit %>%
  conf_mat(truth = survived, estimate = survived_pred)
```

```{r}
# Training accuracy rate
accuracy(pred_train_logit, survived, survived_pred) 
```

__(c) If we want to fit a random forests model using the same formula, how to set the parameter mtry (the number of candidates for splitting varibles)? (5 points)__

#### RESPONDER

Growing a random forest proceeds in exactly the same way, except that
we use a smaller value of the mtry argument. By default, randomForest()
uses p/3 variables when building a random forest of regression trees, and
sqrt(p) variables when building a random forest of classification trees.

__(d) Fit a random forests model and show the confusion matrix for the training data and calculate the training accuracy rate. You can set the number of trees as 201. (10 points).__

```{r}
random_forest_titanic <- randomForest (model, data = training_data,
                                       # mtry = squared of p'
                                       #  -1 = remove response variable
                                       mtry = 
                                         round(sqrt(ncol(training_data) - 1),0), 
                                       
                                       # Number of trees
                                       ntree = 201,
                                       
                                       # Which variables have the greatest 
                                       # importance?
                                       importance = TRUE)

```

```{r}
# Get training predictions
pred_train_random_forest <- 
  training_data %>%
      bind_cols(predict(random_forest_titanic, new_data = titanic[index,])) %>% 
      rename(survived_pred_random_forest = ...9)

# Confusion matrix
pred_train_random_forest %>%
  conf_mat(truth = survived, estimate = survived_pred_random_forest)
```

```{r}
# Training accuracy rate
accuracy(pred_train_random_forest, survived, survived_pred_random_forest)
```

##### Ver diferencia con script de la clase

__(e) Find the most important explanatory variable of the random forests model. (5 points)__
```{r}
varImpPlot(random_forest_titanic)
```

__(f) Find the testing accuracy rate of the two models. (5 points)__

```{r}
# Get test predictions
pred_test_logit <- 
  testing_data %>% 
      bind_cols(predict(model_logit, new_data = testing_data)) %>%
      rename(survived_pred_test = .pred_class)
```

```{r}
# Test accuracy rate
accuracy(pred_test_logit, survived, survived_pred_test)
```

```{r}
# Get test predictions
pred_test_random_forest <- 
  testing_data  %>% 
      bind_cols(predict(random_forest_titanic, newdata = titanic[-index,], type = "class")) %>% 
      rename(survived_pred_random_forest_test = ...9)
      
```

```{r}
# Test accuracy rate
accuracy(pred_test_random_forest, survived, survived_pred_random_forest_test)
```


__(g) Which accuracy should we use to compare the two models? What’s your conclusion? (5 points)__

the testing
__(h) Calculate the sensitivity and specificity of the two models for the testing data. (5 points)__
__(i) (Bonus) Draw the ROC curves of the two models using the testing data. (Hint: when you modify the demo codes, remember to replace ‘.pred >50K‘ with ‘.pred Yes‘.) (5 points)__